import streamlit as st
import requests
from gradio_client import Client
import numpy as np
import soundfile as sf
import sounddevice as sd
import wave
import base64

audio_data = None

# è¯­éŸ³è¯†åˆ«
def recognize_speech(audio_file):
    API_URL = "https://api-inference.huggingface.co/models/openai/whisper-large-v3"
    headers = {"Authorization": "Bearer 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"}

    def recognize(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.post(API_URL, headers=headers, data=data)
        return response.json()

    output = recognize(audio_file)
    return output

# è¯­éŸ³å¢å¼º
def enhance_audio(audio_file):
    EA_URL = "https://api-inference.huggingface.co/models/speechbrain/sepformer-whamr-enhancement"
    headers = {"Authorization": "Bearer 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"}

    def enhance(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.post(EA_URL, headers=headers, data=data)
        return response.json()

    output = enhance(audio_file)
    if output is not None:
        enhanced_audio_data = base64.b64decode(output[0]["blob"])
        return enhanced_audio_data
    else:
        return None;

# TTS
def text_to_speech(text, lang='en'):
    client = Client("https://xzjosh-kobe-bert-vits2-2-3.hf.space/--replicas/9fhp9/")
    example_audio = "./audio/audio_sample.wav"
    result = client.predict(
        text, 
        "ç§‘æ¯”", 
        0.4, 
        0.1,
        0.1,
        2, 
        "EN",
        example_audio, 
        "Angry", 
        "Text prompt",
        "", 
        0, 
        fn_index=0
    )
    audio_path = result[1]
    return audio_path


# å½•éŸ³å‡½æ•°
def record_audio(seconds):
    fs = 44100
    print("å¼€å§‹å½•éŸ³ï¼Œè¯·è¯´è¯...")
    audio_data = sd.rec(int(seconds * fs), samplerate=fs, channels=1, dtype='int16')
    sd.wait()
    print("å½•éŸ³ç»“æŸ.")
    return audio_data

# ä¿å­˜å½•éŸ³æ•°æ®åˆ°WAVæ–‡ä»¶
def save_audio(audio_data, filename):
    wf = wave.open(filename, 'wb')
    wf.setnchannels(1)
    wf.setsampwidth(2)
    wf.setframerate(44100)
    wf.writeframes(audio_data.tobytes())
    wf.close()


# æ˜¾ç¤ºå½•éŸ³æ•°æ®
def display_audio(filename):
    with open(filename, "rb") as audio_file:
        audio_bytes = audio_file.read()
        st.audio(audio_bytes, format="audio/wav")

# éŸ³é¢‘ä¸Šä¼ å‡½æ•°
def upload_audio():
    global audio_data

    uploaded_file = st.file_uploader("ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆè¦†ç›–æ—§çš„å½•éŸ³ï¼‰", type=["wav", "mp3"])
    if uploaded_file is not None:
        # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
        with open("uploaded_audio.wav", "wb") as f:
            f.write(uploaded_file.getvalue())

        # è¯»å–ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶å¹¶ä¿å­˜åˆ°audio_data
        audio_data, _ = sf.read("output.wav")



# Streamlit App
st.title("ğŸ™ï¸ Voice Assistant")

# é€‰æ‹©å¤„ç†ç±»å‹
processing_type = st.radio("é€‰æ‹©å¤„ç†ç±»å‹", ("è¯­éŸ³è¯†åˆ«", "è¯­éŸ³å¢å¼º", "TTS"))

if processing_type == "è¯­éŸ³è¯†åˆ«":
    # é€‰æ‹©å½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    choice = st.radio("è¯·é€‰æ‹©å½•éŸ³æ–¹å¼", ("å½•éŸ³", "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"))

    # å¦‚æœé€‰æ‹©å½•éŸ³
    if choice == "å½•éŸ³":
        seconds_to_record = st.slider("å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰", min_value=1, max_value=10, value=3)
        if st.button("å¼€å§‹å½•éŸ³"):
            # æ‰§è¡Œå½•éŸ³æ“ä½œ
            audio_data = record_audio(seconds_to_record)
                
    # å¦‚æœé€‰æ‹©ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    elif choice == "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶":
        upload_audio()

    if audio_data is not None:
        save_audio(audio_data, "output.wav")
        display_audio("output.wav")
        st.write("æ­£åœ¨è¯†åˆ«è¯­éŸ³ï¼Œè¯·ç¨å€™...")
        recognition_result = recognize_speech("output.wav")
        st.write("è¯†åˆ«ç»“æœï¼š", recognition_result["text"])

elif processing_type == "è¯­éŸ³å¢å¼º":
    # é€‰æ‹©å½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    choice = st.radio("è¯·é€‰æ‹©å½•éŸ³æ–¹å¼", ("å½•éŸ³", "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"))

    # å¦‚æœé€‰æ‹©å½•éŸ³
    if choice == "å½•éŸ³":
        seconds_to_record = st.slider("å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰", min_value=1, max_value=10, value=3)
        if st.button("å¼€å§‹å½•éŸ³"):
            audio_data = record_audio(seconds_to_record)
    # å¦‚æœé€‰æ‹©ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    elif choice == "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶":
        upload_audio()

    # æ˜¾ç¤ºå½•éŸ³ç»“æœ
    if audio_data is not None:
        save_audio("output.wav")
        display_audio("output.wav")

        # æ‰§è¡Œè¯­éŸ³å¢å¼ºå¹¶è·å–ç»“æœ
        st.write("æ­£åœ¨è¿›è¡Œè¯­éŸ³å¢å¼ºï¼Œè¯·ç¨å€™...")
        enhancement_result = enhance_audio("output.wav")
        st.audio(enhancement_result, format='audio/wav')
        
if processing_type == "TTS":
    # è¾“å…¥è¦è½¬æ¢æˆè¯­éŸ³çš„æ–‡æœ¬
    text_input = st.text_input("è¯·è¾“å…¥è¦è½¬æ¢æˆè¯­éŸ³çš„æ–‡æœ¬")
    # ç¡®è®¤æŒ‰é’®
    submit_button = st.button("ç¡®è®¤æäº¤")
    
    if submit_button and text_input:
        # è°ƒç”¨text_to_speechå‡½æ•°ç”Ÿæˆè¯­éŸ³
        audio_path = text_to_speech(text_input)
        # æ’­æ”¾ç”Ÿæˆçš„è¯­éŸ³
        st.audio(audio_path, format='audio/wav')
    