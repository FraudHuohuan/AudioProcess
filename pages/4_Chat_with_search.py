import streamlit as st
import requests
from gradio_client import Client
import numpy as np
import soundfile as sf
from audio_recorder_streamlit import audio_recorder
import base64

# è¯­éŸ³è¯†åˆ«
def recognize_speech(audio_file):
    API_URL = "https://api-inference.huggingface.co/models/openai/whisper-large-v3"
    headers = {"Authorization": "Bearer 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"}

    def recognize(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.post(API_URL, headers=headers, data=data)
        return response.json()

    output = recognize(audio_file)
    return output

# è¯­éŸ³å¢å¼º
def enhance_audio(audio_file):
    EA_URL = "https://api-inference.huggingface.co/models/speechbrain/sepformer-whamr-enhancement"
    headers = {"Authorization": "Bearer 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"}

    def enhance(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.post(EA_URL, headers=headers, data=data)
        return response.json()

    output = enhance(audio_file)
    if output is not None:
        enhanced_audio_data = base64.b64decode(output[0]["blob"])
        return enhanced_audio_data
    else:
        return None;

# TTS
def text_to_speech(text, lang='en'):
    client = Client("https://xzjosh-kobe-bert-vits2-2-3.hf.space/--replicas/9fhp9/")
    example_audio = "./audio/audio_sample.wav"
    result = client.predict(
        text, 
        "ç§‘æ¯”", 
        0.4, 
        0.1,
        0.1,
        2, 
        "EN",
        example_audio, 
        "Angry", 
        "Text prompt",
        "", 
        0, 
        fn_index=0
    )
    audio_path = result[1]
    return audio_path


# Streamlit App
st.title("ğŸ™ï¸ Voice Assistant")

# é€‰æ‹©å¤„ç†ç±»å‹
processing_type = st.radio("é€‰æ‹©å¤„ç†ç±»å‹", ("è¯­éŸ³è¯†åˆ«", "è¯­éŸ³å¢å¼º", "TTS"))

if processing_type == "è¯­éŸ³è¯†åˆ«":
    # é€‰æ‹©å½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    choice = st.radio("è¯·é€‰æ‹©å½•éŸ³æ–¹å¼", ("å½•éŸ³", "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"))

    # å¦‚æœé€‰æ‹©å½•éŸ³
    if choice == "å½•éŸ³":
       seconds_to_record = st.slider("å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰", min_value=1, max_value=10, value=3)
       if st.button("å¼€å§‹å½•éŸ³"):
           record_audio(seconds_to_record)
                
    # å¦‚æœé€‰æ‹©ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    elif choice == "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶":
        upload_audio()

    if st.session_state.audio_data is not None:
        save_audio(st.session_state.audio_data, "output.wav")
        st.audio("output.wav", format="audio/wav")
        st.write("æ­£åœ¨è¯†åˆ«è¯­éŸ³ï¼Œè¯·ç¨å€™...")
        recognition_result = recognize_speech("output.wav")
        st.write("è¯†åˆ«ç»“æœï¼š", recognition_result["text"])

elif processing_type == "è¯­éŸ³å¢å¼º":
    # é€‰æ‹©å½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    choice = st.radio("è¯·é€‰æ‹©å½•éŸ³æ–¹å¼", ("å½•éŸ³", "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"))

    # å¦‚æœé€‰æ‹©å½•éŸ³
    if choice == "å½•éŸ³":
        seconds_to_record = st.slider("å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰", min_value=1, max_value=10, value=3)
        if st.button("å¼€å§‹å½•éŸ³"):
            record_audio(seconds_to_record)
    # å¦‚æœé€‰æ‹©ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    elif choice == "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶":
        upload_audio()

    # æ˜¾ç¤ºå½•éŸ³ç»“æœ
    if audio_data is not None:
        save_audio("output.wav")
        display_audio()

        # æ‰§è¡Œè¯­éŸ³å¢å¼ºå¹¶è·å–ç»“æœ
        st.write("æ­£åœ¨è¿›è¡Œè¯­éŸ³å¢å¼ºï¼Œè¯·ç¨å€™...")
        enhancement_result = enhance_audio("output.wav")
        st.audio(enhancement_result, format='audio/wav')
        
if processing_type == "TTS":
    # è¾“å…¥è¦è½¬æ¢æˆè¯­éŸ³çš„æ–‡æœ¬
    text_input = st.text_input("è¯·è¾“å…¥è¦è½¬æ¢æˆè¯­éŸ³çš„æ–‡æœ¬")
    # ç¡®è®¤æŒ‰é’®
    submit_button = st.button("ç¡®è®¤æäº¤")
    
    if submit_button and text_input:
        # è°ƒç”¨text_to_speechå‡½æ•°ç”Ÿæˆè¯­éŸ³
        audio_path = text_to_speech(text_input)
        # æ’­æ”¾ç”Ÿæˆçš„è¯­éŸ³
        st.audio(audio_path, format='audio/wav')


    